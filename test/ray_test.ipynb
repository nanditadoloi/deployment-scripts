{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99031272-d0be-407d-83ff-f3d5556c4215",
   "metadata": {},
   "source": [
    "#!pip install ray[client,train]\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "#!pip install -U ipywidgets\n",
    "#!pip install pyarrow==16.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f0137-67a8-42e2-b311-e7eda3235b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 16:44:13,046\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "2024-11-27 16:44:17,674\tWARNING utils.py:1591 -- Python patch version mismatch: The cluster was started with:\n",
      "    Ray: 2.39.0\n",
      "    Python: 3.10.15\n",
      "This process on Ray Client was started with:\n",
      "    Ray: 2.39.0\n",
      "    Python: 3.10.12\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node:10.244.15.2': 1.0, 'node:__internal_head__': 1.0, 'CPU': 6.0, 'object_store_memory': 1812653260.0, 'node:10.244.14.8': 1.0, 'memory': 6442450944.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m View detailed results here: /tmp/ray/test_experiment\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-11-27_15-28-54_841828_1/artifacts/2024-11-27_16-44-47/test_experiment/driver_artifacts`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=13587, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(pid=13587, ip=10.244.15.2)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TrainTrainable pid=13587, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(TrainTrainable pid=13587, ip=10.244.15.2)\u001b[0m   _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m Training started with configuration:\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m ╭────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ Training config                    │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m ├────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ train_loop_config/input_size    10 │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ train_loop_config/layer_size    15 │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ train_loop_config/num_epochs     3 │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ train_loop_config/num_samples   20 │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m │ train_loop_config/output_size    5 │\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m ╰────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m   _torch_pytree._register_pytree_node(\n",
      "\u001b[36m(TorchTrainer pid=13587, ip=10.244.15.2)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=13587, ip=10.244.15.2)\u001b[0m - (node_id=2c3909747cfbec0742812befe5e6edab0082845665689f4a60b19d03, ip=10.244.15.2, pid=13676) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m   _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s] \u001b[0m \n",
      "Epoch 2: 100%|██████████| 5/5 [00:00<00:00, 206.56it/s, loss=1.04, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m Missing logger folder: /tmp/ray/session_2024-11-27_15-28-54_841828_1/artifacts/2024-11-27_16-44-47/test_experiment/working_dirs/TorchTrainer_f79e7_00000_0_2024-11-27_16-44-48/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m   | Name    | Type    | Params\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m ------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 0 | layer1  | Linear  | 165   \n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 1 | relu    | ReLU    | 0     \n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 2 | layer2  | Linear  | 80    \n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 3 | loss_fn | MSELoss | 0     \n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m ------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 245       Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 245       Total params\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m 0.001     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m   rank_zero_warn(\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m   rank_zero_warn(\n",
      "\u001b[36m(RayTrainWorker pid=13676, ip=10.244.15.2)\u001b[0m `Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m Training completed after 0 iterations at 2024-11-27 16:45:22. Total running time: 34s\n",
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=18941)\u001b[0m Wrote the latest version of all result files and experiment state to '/tmp/ray/test_experiment' in 0.0046s.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "ray.init(address=\"ray://localhost:10001\")\n",
    "\n",
    "# Verify the connection\n",
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837c1806-5239-4912-bc0f-87de124ea094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LightningNeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self, input_size, layer_size, output_size):\n",
    "        super(LightningNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.relu(self.layer1(x)))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b105f53f-1186-4a67-9f15-804dc692235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import train\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "from ray.air.config import RunConfig\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_func(config):\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer\n",
    "\n",
    "    # Initialize the Lightning Module\n",
    "    model = LightningNeuralNetwork(\n",
    "        input_size=config[\"input_size\"],\n",
    "        layer_size=config[\"layer_size\"],\n",
    "        output_size=config[\"output_size\"]\n",
    "    )\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.randn(config[\"num_samples\"], config[\"input_size\"]),\n",
    "        torch.randn(config[\"num_samples\"], config[\"output_size\"])\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "    # Initialize PyTorch Lightning Trainer\n",
    "    lightning_trainer = Trainer(\n",
    "        max_epochs=config[\"num_epochs\"],\n",
    "        accelerator='auto',  # Automatically use GPU if available\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    lightning_trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09579f-d632-4b36-92df-206c2edd9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the connection\n",
    "print(ray.cluster_resources())\n",
    "\n",
    "# Define configuration parameters\n",
    "config = {\n",
    "    \"num_samples\": 20,\n",
    "    \"input_size\": 10,\n",
    "    \"layer_size\": 15,\n",
    "    \"output_size\": 5,\n",
    "    \"num_epochs\": 3,\n",
    "}\n",
    "\n",
    "# Define the trainer with ScalingConfig\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=1,\n",
    "        trainer_resources={\"CPU\": 1},\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    train_loop_config=config,\n",
    "    run_config = RunConfig(storage_path=\"/tmp/ray\", name=\"test_experiment\")\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee27df-ee13-44e5-a4b1-8032c26c76f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
